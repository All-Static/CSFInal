{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train = r'C:\\Users\\Steven\\OneDrive - Cal State LA\\Documents\\train_metadata.json'\n",
    "with open(data_path_train) as json_file:\n",
    "    meta_train = json.load(json_file)\n",
    "    \n",
    "    \n",
    "#loading json file for test data\n",
    "data_path_test = r'C:\\Users\\Steven\\OneDrive - Cal State LA\\Documents\\test_metadata.json'\n",
    "with open(data_path_test) as json_file:\n",
    "    meta_test = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['annotations', 'images', 'categories', 'genera', 'institutions', 'distances', 'license'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['annotations', 'images', 'categories', 'genera', 'institutions', 'distances', 'license'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding keys in train dictionary\n",
    "meta_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating seperate dataframes from metadata\n",
    "annotations_train =  pd.json_normalize(meta_train ['annotations'])\n",
    "categories_train =  pd.json_normalize(meta_train ['categories'])\n",
    "images_train =  pd.json_normalize(meta_train ['images'])\n",
    "genera_train =  pd.json_normalize(meta_train ['genera'])\n",
    "distance_train =  pd.json_normalize(meta_train ['distances'])\n",
    "licenses_train =  pd.json_normalize(meta_train ['license'])\n",
    "institutions_train =  pd.json_normalize(meta_train ['institutions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del genera_train\n",
    "del licenses_train\n",
    "del institutions_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at test set\n",
    "df_test = pd.DataFrame(meta_test)\n",
    "\n",
    "#creating test data\n",
    "df_test = df_test.drop(['license'], axis=1)\n",
    "\n",
    "print(df_test['file_name'])\n",
    "# adding file path\n",
    "df_test = df_test[['image_id','file_name']]\n",
    "df_test['file_path']='C:/Users/Steven/OneDrive - Cal State LA/Documents/image/'+df_test['file_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete df\n",
    "df_merge = pd.merge(images_train[['image_id','file_name']],annotations_train[['genus_id','category_id','image_id']] , on='image_id')\n",
    "df_merge = pd.merge(df_merge[['genus_id','image_id','file_name','category_id']],categories_train[['category_id','scientificName','family','genus','species']] , on='category_id')\n",
    "df_merge['file_path']='C:/Users/Steven/OneDrive - Cal State LA/Documents/images2/'+df_merge['file_name']\n",
    "df_merge['name']=df_merge['genus']+' '+df_merge['species']\n",
    "df_train = df_merge[['category_id','genus_id','image_id','family','genus','species','name','file_name','file_path']]\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "y=[]\n",
    "def makeimage(i2,img2):\n",
    "    image=mpimg.imread(df_train['file_path'][i2])\n",
    "    #img=imread(df_train['file_path'][i2],as_gray=True)\n",
    "    #img=img[0:665,0:595]\n",
    "    #img2.append(np.reshape(img, (665*595)))\n",
    "    y.append(df_train['category_id'][i2])\n",
    "    feature_matrix = np.zeros((665,595) )\n",
    "    feature_matrix.shape\n",
    "    \n",
    "    for i in range(0,665):\n",
    "        for j in range(0,595):\n",
    "            feature_matrix[i][j] = ((int(image[i,j,0]) + int(image[i,j,1]) + int(image[i,j,2]))/3)\n",
    "    \n",
    "    img2.append(np.reshape(feature_matrix, (665*595))) \n",
    "    #features.shape\n",
    "\n",
    "    #print(features)\n",
    "\n",
    "\n",
    "i2=0\n",
    "img2=[]\n",
    "while i2<500:\n",
    "    makeimage(i2,img2)\n",
    "    i2=i2+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img2=np.array(img2)\n",
    "print(np.array(img2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dff=pd.DataFrame(y,columns = ['Name'])\n",
    "y=dff['Name']\n",
    "print(y)\n",
    "from sklearn import preprocessing\n",
    "X=img2\n",
    "#c\n",
    "X=preprocessing.scale(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed= 1\n",
    "yes=np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=111)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# for getting accuracy of our classifiers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#my_decisiontree = DecisionTreeClassifier(random_state=3)       \n",
    "#my_decisiontree.fit(X_train, y_train)\n",
    "#y_predict = my_decisiontree.predict(X_test)\n",
    "\n",
    "\n",
    "#my_random_forest= RandomForestClassifier(n_estimators=100, bootstrap=True, n_jobs=-1, random_state=111)\n",
    "#my_random_forest.fit(X_train, y_train)\n",
    "#y_predict =my_random_forest.predict(X_test)\n",
    "\n",
    "#my_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=4) \n",
    "#my_knn_for_cs4661.fit(X_train, y_train)\n",
    "#y_predict = my_knn_for_cs4661.predict(X_test)\n",
    "#score_lr = accuracy_score(y_test, y_predict)\n",
    "#print(score_lr)\n",
    "#model = MLPClassifier( random_state=111, hidden_layer_sizes=(10,),learning_rate_init = 0.05, solver='adam', alpha=1, verbose=True, activation='logistic')\n",
    "from keras.models import Sequential\n",
    "from  keras.wrappers.scikit_learn  import  KerasClassifier\n",
    "from  sklearn.model_selection  import  GridSearchCV\n",
    "# import the core layers:\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "def model_creator():\n",
    "\n",
    "    # define:\n",
    "\n",
    "    \n",
    "\n",
    "    # design the structure:\n",
    "\n",
    "    ## Designing the ANN Structure (with 784 inputs, 10 outputs and 100 neuron in a hidden layer):\n",
    "    # Declare Sequential model to build our network:\n",
    "    model = Sequential()\n",
    "    input_size = 665*595\n",
    "    hidden_neurons = 100\n",
    "    out_size = 10\n",
    "    # -----------------------------------------\n",
    "    # first layer: input layer\n",
    "    # Input layer does not do any processing, so no need to define the input layer in this problem.\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # second layer: hidden layer:\n",
    "    model.add(Dense(hidden_neurons, input_dim = input_size))  # Nuerons\n",
    "    model.add(Activation('sigmoid')) # Activation\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # third layer: output layer:\n",
    "    model.add(Dense(out_size, input_dim = hidden_neurons))  # Nuerons\n",
    "    model.add(Activation('softmax')) # Activation\n",
    "\n",
    "    # compile:\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')\n",
    "\n",
    "    # return: \n",
    "\n",
    "    return model \n",
    "\n",
    "seed= 2\n",
    "np.random.seed(seed)\n",
    "model = KerasClassifier(build_fn = model_creator, verbose=2)\n",
    "batch_size = [30 , 50 , 100 ] \n",
    "epochs = [10 , 15 , 20]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "grid=GridSearchCV(estimator=model,param_grid=param_grid,scoring='neg_log_loss',cv=10)\n",
    "fitted_model = grid.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (fitted_model.best_score_, fitted_model.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fitted_model2 = model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction:\n",
    "model.predict(X_test)\n",
    "model.score(X_test,y_test)\n",
    "#cm=confusion_matrix(y_predict, y_test)\n",
    "#print('The accuracy is: ', accuracy(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from  sklearn.model_selection  import  GridSearchCV\n",
    "\n",
    "seed= 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "neurons = [] \n",
    "\n",
    "i=5\n",
    "\n",
    "while i<=250:\n",
    "    neurons.append(i)\n",
    "    i=i+5\n",
    "print(neurons)\n",
    "\n",
    "\n",
    "lst_tuple = [x for x in zip(*[iter(neurons)])]\n",
    "print(lst_tuple)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "GRID = [\n",
    "    {\n",
    "     'estimator': [MLPClassifier(random_state=seed)],\n",
    "     'estimator__solver': ['adam'],\n",
    "     'estimator__learning_rate_init': [0.02],\n",
    "     'estimator__hidden_layer_sizes': lst_tuple,\n",
    "     'estimator__activation': ['logistic'],\n",
    "     'estimator__alpha': [1]\n",
    "     }\n",
    "]\n",
    "\n",
    "PIPELINE = Pipeline([('scaler', None), ('estimator', MLPClassifier())])\n",
    "\n",
    "grid_search = GridSearchCV(estimator=PIPELINE, param_grid=GRID, \n",
    "                            scoring='accuracy', cv=10, verbose=True, \n",
    "                            )\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
